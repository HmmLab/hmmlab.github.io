
<!DOCTYPE HTML PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <title>HmmLab</title>

    <link rel="icon" href="favicon.ico" type="image/x-icon" />
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
    <link rel="bookmark" href="favicon.ico" type="image/x-icon" />
    <link rel="stylesheet" type="text/css" href="main.css" />
    <link rel="stylesheet" type="text/css" href="index.css" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-TXEVECV8DQ"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-TXEVECV8DQ');
    </script>
</head>
<body>
    <div id="wrapper">
        <div id="heading"></div>
        <div id="aboutme" class="section">
            <div id="aboutmesectitle" class="sectitle">[About lAB]</div>
            <div id="aboutmecontent" class="seccontent">
                <div id="mypic">
                    <!-- <a href="index.html">
                        <img src="imgs/me.jpg" alt="Libin Liu" />
                    </a> -->
                </div>
                <div id="myinfo">
                    <p id="myname">
                        HmmLab
                    </p>
                    <!-- <p id="myaffiliation">
                        Assistant Professor
                        <br /><br />
                        <a class="institute" href="https://www.cis.pku.edu.cn/" target="_blank">
                            School of Intelligence Science and Technology
                        </a>
                        <br />
                        <a class="institute" href="http://english.pku.edu.cn/" target="_blank">Peking University</a>
                    </p> -->
                    <!-- <p id="contacttitle"></p>
                    <p class="mycontact">
                        Email: libin.liu [at] pku.edu.cn
                    </p> -->
                </div>
                <div id="myintro">
                    <p>
                        实验室简介：<br/>
                        
                        <br />
                    </p>
                </div>
            </div>
        </div>

        <div id="project" class="section">
            <div id="projsectitle" class="sectitle">[Projects]</div>
            <div id="projects" class="seccontent">            
                <div id="Motif" class="projitem">
                    <div id="Motif" class="projthumbnail">
                        <a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page" target="_self">
                            <img src="https://HmmLab.github.io/works/image/Motif.gif" alt="Motif" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <a class="papertitle" href="https://github.com/wenyh1616/SAMotif-GCN">
                                Motif-GCNs With Local and Non-Local Temporal Blocks for Skeleton-Based Action Recognition.
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Yu-Hui Wen, Lin Gao, Hongbo Fu, Fang-Lue Zhang, Shihong Xia, Yong-Jin Liu -->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089016300" target="_blank">Yu-Hui </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37085738359" target="_blank">Lin Gao </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37626240300" target="_blank">Hongbo Fu </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/38474069400" target="_blank">Fang-Lue Zhang </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37288203400" target="_blank">Shihong Xia </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37279426700" target="_blank">Yong-Jin Liu</a>,
                        </p>

                        <p class="projbrief">
                            Recent works have achieved remarkable performance for action recognition with human skeletal data by utilizing graph convolutional models. Existing models mainly focus on developing graph convolutional operations to encode structural properties of a skeletal graph,
                            whose topology is manually predefined and fixed over all action samples. Some recent works further take sample-dependent relationships among joints into consideration. However, the complex relationships between arbitrary pairwise joints are difficult to learn and the temporal features between frames are not fully exploited by simply using traditional convolutions with small local kernels.
                            In this paper, we propose a motif-based graph convolution method, which makes use of sample-dependent latent relations among non-physically connected joints to impose a high-order locality and assigns different semantic roles to physical neighbors of a joint to encode hierarchical structures. Furthermore, we propose a sparsity-promoting loss function to learn a sparse motif adjacency matrix for latent dependencies in non-physical connections. 
                            For extracting effective temporal information, we propose an efficient local temporal block. It adopts partial dense connections to reuse temporal features in local time windows, and enrich a variety of information flow by gradient combination.
                            In addition, we introduce a non-local temporal block to capture global dependencies among frames. Our model can capture local and non-local relationships both spatially and temporally, by integrating the local and non-local temporal blocks into the sparse motif-based graph convolutional networks (SMotif-GCNs).
                            Comprehensive experiments on four large-scale datasets show that our model outperforms the state-of-the-art methods. 
                            Our code is publicly available at https://github.com/wenyh1616/SAMotif-GCN .
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank">IEEE Transactions on Pattern Analysis and Machine Intelligence </a>( Volume: 45, Issue: 2, 01 February 2023)
                        </p>
                        <p class="projlinks">
                            [<a href="https://github.com/wenyh1616/SAMotif-GCN">Project Page</a>]
                            [<a href="https://HmmLab.github.io/works/papers/Motif-GCNs_With_Local_and_Non-Local_Temporal_Blocks_for_Skeleton-Based_Action_Recognition.pdf">Paper</a>]
                            <!-- [Video (<a href="https://www.youtube.com/watch?v=qy2MrNhsoIs">YouTube</a>|<a href="https://www.bilibili.com/video/BV1G24y1d7Tt">BiliBili</a>)] -->
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>
                
                <div id="Multiscale" class="projitem">
                    <div id="Multiscale" class="projthumbnail">
                        <a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page" target="_self">
                            <img src="image/Multiscale Mesh.gif" alt="Motif" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/9537699">
                                Multiscale Mesh Deformation Component Analysis With Attention-Based Autoencoders
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Jie Yang; Lin Gao; Qingyang Tan; Yi-Hua Huang; Shihong Xia; Yu-Kun Lai -->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088450605" target="_blank">Jie Yang </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37085738359" target="_blank">Lin Gao </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37086567528" target="_blank">Qingyang Tan </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089542762" target="_blank">Yi-Hua Huang </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37288203400" target="_blank">Shihong Xia </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/38575316500" target="_blank">Yu-Kun Lai</a>,
                        </p>

                        <p class="projbrief">
                            Deformation component analysis is a fundamental problem in geometry processing and shape understanding. 
                            Existing approaches mainly extract deformation components in local regions at a similar scale while deformations of real-world objects are usually distributed in a multi-scale manner. 
                            In this article, we propose a novel method to exact multiscale deformation components automatically with a stacked attention-based autoencoder. 
                            The attention mechanism is designed to learn to softly weight multi-scale deformation components in active deformation regions, and the stacked attention-based autoencoder is learned to represent the deformation components at different scales.
                            Quantitative and qualitative evaluations show that our method outperforms state-of-the-art methods. Furthermore, with the multiscale deformation components extracted by our method, 
                            the user can edit shapes in a coarse-to-fine fashion which facilitates effective modeling of new shapes.
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                IEEE Transactions on Visualization and Computer Graphics</a> ( Volume: 29, Issue: 2, 01 February 2023)
                        </p>
                        <p class="projlinks">
                            [Project Page]<!-- [<a href="https://github.com/wenyh1616/SAMotif-GCN">Project Page</a>] -->
                            [<a href="https://HmmLab.github.io/works/papers/Multiscale_Mesh_Deformation_Component_Analysis_wit.pdf">Paper</a>]
                            [<a href="video/Multiscale Mesh.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <div id="Pose-aware" class="projitem">
                    <div id="Pose-aware" class="projthumbnail">
                        <a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page" target="_self">
                            <img src="image/Pose-aware.png" alt="Pose-aware" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://arxiv.org/abs/2306.08006">
                                Pose-aware Attention Network for Flexible Motion Retargeting by Body Part
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Lei Hu, Zihao Zhang, Chongyang Zhong, Boyuan Jiang, Shihong Xia -->
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">lei hu </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37085738359" target="_blank">Lin Gao </a>,
                            <a class="name" href="http://zihaozhang.tech/" target="_blank">Zihao Zhang</a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089543322" target="_blank">Chongyang Zhong </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37086479093" target="_blank">Boyuan Jiang</a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37288203400" target="_blank">Shihong Xia </a>,
                            
                        </p>

                        <p class="projbrief">
                            Motion retargeting is a fundamental problem in computer graphics and computer vision. 
                            Existing approaches usually have many strict requirements, such as the source-target skeletons needing to have the same number of joints or share the same topology.
                            To tackle this problem, we note that skeletons with different structure may have some common body parts despite the differences in joint numbers. Following this observation,
                            we propose a novel, flexible motion retargeting framework. The key idea of our method is to regard the body part as the basic retargeting unit rather than directly retargeting the whole body motion. 
                            To enhance the spatial modeling capability of the motion encoder,we introduce a pose-aware attention network (PAN) in the motion encoding phase. The PAN is pose-aware since it can dynamically predict the joint weights within each body part based on the input pose,
                            and then construct a shared latent space for each body part by feature pooling. Extensive experiments show that our approach can generate better motion retargeting results both qualitatively and quantitatively than state-of-the-art methods. Moreover, we also show that our framework can generate reasonable results even for a more challenging retargeting scenario, like retargeting between bipedal and quadrupedal skeletons because of the body part retargeting strategy and PAN. Our code is publicly available.
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                
                                IEEE Transactions on Visualization and Computer Graphics</a> (17 pages, 12 figures. 2023)
                        </p>
                        <p class="projlinks">
                            [<a href="https://github.com/hlcdyy/pan-motion-retargeting">Project Page</a>]
                            [<a href="https://HmmLab.github.io/works/papers/Pose-aware Attention Network for Flexible Motion Retargeting by Body Part.pdf">Paper</a>]
                            [<a href="https://youtu.be/oTAcxTtPXUg">YouTube</a>|<a href="https://www.bilibili.com/video/BV1bg4y1V7Xi/">BiliBili</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>
                

                
            </div>
        </div>

        <!-- <div id="activity" class="section">
            <div id="activitysectitle" class="sectitle">[Professional Activities]</div>
            <div id="activities" class="seccontent">
                <div class="activitytype">
                    Program Committee:
                    <ul>
                        <li>ACM SIGGRAPH North America 2019, 2020, Asia 2022</li>
                        <li>Pacific Graphics 2018, 2019, 2022</li>
                        <li>SCA 2015-2019, 2021-2023</li>
                        <li>MIG 2014, 2016-2019, 2022</li>
                        <li>Eurographics Short Papers 2020, 2021</li>
                        <li>SIGGRAPH Asia 2014 Posters and Technical Briefs</li>
                        <li>CASA (Computer Animation and Social Agents) 2017, 2023</li>
                        <li>Graphics Interface 2023</li>
                        <li>CAD/Graphics 2017, 2019</li>
                    </ul>
                </div> 
                <div class="activitytype">
                    Paper Reviewing:
                    <ul>
                        <li>SIGGRAPH NA/Asia</li>
                        <li>ACM Transactions on Graphics (TOG)</li>
                        <li>IEEE Transactions on Visualization and Computer Graphics (TVCG)</li>
                        <li>International Conference on Computer Vision (ICCV)</li>
                        <li>Eurographics (Eupopean Association for Computer Graphics)</li>
                        <li>Pacific Graphics</li>
                        <li>Computer Graphics Forum</li>
                        <li>IEEE International Conference on Robotics and Automation (ICRA)</li>
                        <li>ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA)</li>
                        <li>ACM SIGGRAPH Conference on Motion, Interaction and Games (MIG)</li>
                        <li>Computer Animation and Social Agents (CASA)</li>
                        <li>Graphics Interface</li>
                        <li>Computers & Graphics</li>
                        <li>Graphical Models</li>
                    </ul>
                </div> 
        </div>
        <div id="footing"></div>
    </div> -->
</body>
</html>