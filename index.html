
<!DOCTYPE HTML PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <title>HmmLab</title>
    <link rel="icon" href="favicon.ico" type="image/x-icon" />
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
    <link rel="bookmark" href="favicon.ico" type="image/x-icon" />
    <link rel="stylesheet" type="text/css" href="main.css" />
    <link rel="stylesheet" type="text/css" href="index.css" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-TXEVECV8DQ"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-TXEVECV8DQ');
    </script>
</head>
<body>
    
    <div id="wrapper">
        <div id="ict" class="ict" >
            <a href="image/ict.png"  target="_self">
                <img src="image/ict.png"  alt="ict" />
            </a>
        </div>
        <div id="heading"></div>
        <div id="aboutme" class="section">
            <div id="aboutmesectitle" class="sectitle">[About LAB]</div>
            <div id="aboutmecontent" class="seccontent">
                <div id="mypic">
                    <a href="xiashihong.webp">
                        <img src="image/xiashihong.webp" alt="ShiHong X" />
                    </a>
                </div>
                <div id="myinfo">
                    <p id="myname">
                        HmmLab
                    </p>
                    <p id="myaffiliation">
                        <b>夏时洪</b>
                        <br /><br />
                        Professor
                        <br /><br />
                        <a class="institute" href="http://www.ict.cas.cn/" target="_blank">
                            Institute of Computing Technology
                        </a>
                        <br />
                     </p>
                    <p id="contacttitle"></p>
                    <p class="mycontact">
                        Email: xsh@ict.ac.cn
                    </p>
                </div>
                <div id="myintro" class="myinfo"  target="_blank">
                    <p>
                        实验室简介：<br/>
                        主要在计算机应用方向从事人体运动建模仿真基础交叉研究，涉及计算机图形学、虚拟现实、人工智能。
                        具体研究兴趣包括基于深度学习的运动捕获与运动行为理解、基于生成式模型的运动合成与运动控制、基于数字孪生的虚拟人建模，以及这些技术在增强现实、人形机器人等领域的应用。
                        <br />
                    </p>
                </div>
            </div>
        </div>

        <div id="project" class="section">
            <div id="projsectitle" class="sectitle">[Projects]</div>
            <div id="projects" class="seccontent">  

                <div id="projects" class="section"> [2023] </div>

                <!-- Pose-aware Attention Network for Flexible Motion Retargeting by Body Part -->
                <div id="Pose-aware" class="projitem">
                    <div id="Pose-aware" class="projthumbnail">
                        <a href="himage/Pose-aware.png" target="_self">
                            <img src="image/Pose-aware.png" alt="Pose-aware" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://arxiv.org/abs/2306.08006">
                                Pose-aware Attention Network for Flexible Motion Retargeting by Body Part
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Lei Hu, Zihao Zhang, Chongyang Zhong, Boyuan Jiang, Shihong Xia -->
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">lei hu </a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao </a>,
                            <a class="name" href="http://zihaozhang.tech/" target="_blank">Zihao Zhang</a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089543322" target="_blank">Chongyang Zhong </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37086479093" target="_blank">Boyuan Jiang</a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                IEEE Transactions on Visualization and Computer Graphics</a> (Volume: 10,26 July 2022)
                        </p>
                        <p class="projlinks">
                            [<a href="https://github.com/hlcdyy/pan-motion-retargeting">Project Page</a>]
                            [<a href="https://HmmLab.github.io/works/papers/Pose-aware Attention Network for Flexible Motion Retargeting by Body Part.pdf">Paper</a>]
                            [<a href="https://youtu.be/oTAcxTtPXUg">YouTube</a>|<a href="https://www.bilibili.com/video/BV1bg4y1V7Xi/">BiliBili</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--Towards Fine-grained 3D Face Dense Registration: An Optimal Dividing and Diffusing Method. -->
                <div id="Towards_Fine-grained" class="projitem">                   
                    <div id="Towards_Fine-grained" class="projthumbnail">
                        <a href="image/Towards_Fine-grained.png" target="_self">
                            <img src="image/Towards_Fine-grained.png" alt="Towards_Fine-grained" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://arxiv.org/abs/2109.11204">
                                Towards Fine-grained 3D Face Dense Registration: An Optimal Dividing and Diffusing Method
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Zhenfeng Fan, Silong Peng, Shihong Xia -->
                            <a href="https://github.com/NaughtyZZ">Zhenfeng Fan </a>,
                            <a >Silong Peng </a>, 
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                        </p>
                        <p class="projconference">
                            <a >International Journal of Computer Vision</a> 
                        <p class="projlinks">
                            [<a href="https://github.com/NaughtyZZ/3D_face_dense_registration">Project Page</a>]
                            [<a href="papers/Towards Fine-grained 3D Face Dense Registration_An Optimal Dividing and Diffusing Method.pdf">Paper</a>]
                            [<a >Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!-- Motif-GCNs With Local and Non-Local Temporal Blocks for Skeleton-Based Action Recognition. -->
                <div id="Motif" class="projitem">
                    <div id="Motif" class="projthumbnail">
                        <a href="https://HmmLab.github.io/works/image/Motif.gif" target="_self">
                            <img src="https://HmmLab.github.io/works/image/Motif.gif" alt="Motif" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <a class="papertitle" href="https://github.com/wenyh1616/SAMotif-GCN">
                                Motif-GCNs With Local and Non-Local Temporal Blocks for Skeleton-Based Action Recognition.
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Yu-Hui Wen, Lin Gao, Hongbo Fu, Fang-Lue Zhang, Shihong Xia, Yong-Jin Liu -->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089016300" target="_blank">Yu-Hui </a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37626240300" target="_blank">Hongbo Fu </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/38474069400" target="_blank">Fang-Lue Zhang </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37279426700" target="_blank">Yong-Jin Liu</a>
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank">
                                IEEE Transactions on Pattern Analysis and Machine Intelligence </a>( Volume: 45, Issue: 2, 01 February 2023)
                        </p>
                        <p class="projlinks">
                            [<a href="https://github.com/wenyh1616/SAMotif-GCN">Project Page</a>]
                            [<a href="https://HmmLab.github.io/works/papers/Motif-GCNs_With_Local_and_Non-Local_Temporal_Blocks_for_Skeleton-Based_Action_Recognition.pdf">Paper</a>]
                            <!-- [Video (<a href="https://www.youtube.com/watch?v=qy2MrNhsoIs">YouTube</a>|<a href="https://www.bilibili.com/video/BV1G24y1d7Tt">BiliBili</a>)] -->
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>
                
                <!-- Multiscale Mesh Deformation Component Analysis With Attention-Based Autoencoders -->
                <div id="Multiscale" class="projitem">
                    <div id="Multiscale" class="projthumbnail">
                        <a href="image/Multiscale Mesh.gif" target="_self">
                            <img src="image/Multiscale Mesh.gif" alt="Motif" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/9537699">
                                Multiscale Mesh Deformation Component Analysis With Attention-Based Autoencoders
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Jie Yang; Lin Gao; Qingyang Tan; Yi-Hua Huang; Shihong Xia; Yu-Kun Lai -->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088450605" target="_blank">Jie Yang </a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37086567528" target="_blank">Qingyang Tan </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089542762" target="_blank">Yi-Hua Huang </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/38575316500" target="_blank">Yu-Kun Lai</a>
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                IEEE Transactions on Visualization and Computer Graphics</a> (Volume: 29, Issue: 2, 01 February 2023)
                        </p>
                        <p class="projlinks">
                            [Project Page]<!-- [<a href="https://github.com/wenyh1616/SAMotif-GCN">Project Page</a>] -->
                            [<a href="https://HmmLab.github.io/works/papers/Multiscale_Mesh_Deformation_Component_Analysis_wit.pdf">Paper</a>]
                            [<a href="video/Multiscale Mesh.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <div id="projects" class="section"> [2022] </div>
                <!--Spatial-temporal modeling for prediction of stylized human motion. -->
                <div id="Spatio-temporal" class="projitem">                   
                    <div id="Spatio-temporal" class="projthumbnail">
                        <a href="image/Spatio-temporal.jpg" target="_self">
                            <img src="image/Spatio-temporal.jpg" alt="Spatio-temporal" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://www.sciencedirect.com/science/article/pii/S092523122201075X">
                                Spatial-temporal modeling for prediction of stylized human motion
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Chongyang Zhong, Lei Hu, Shihong Xia-->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089543322" target="_blank">Chongyang Zhong </a>,
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">lei hu </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                            
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://www.sciencedirect.com/journal/neurocomputing" target="_blank">
                                Neurocomputing </a> ( Volume 511, 28 October 2022, Pages 34-42)
                        <p class="projlinks">
                            [<a>Project Page</a>]
                            [<a href="papers/Spatial–temporal modeling for prediction of stylized human motion.pdf">Paper</a>]
                            [<a>Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--Learning Uncoupled-Modulation CVAE for 3D Action-Conditioned Human Motion Synthesis. -->
                <div id="Learning_Uncoupled-Modulation" class="projitem">                   
                    <div id="Learning_Uncoupled-Modulation" class="projthumbnail">
                        <a href="image/Learning_Uncoupled-Modulation.png" target="_self">
                            <img src="image/Learning_Uncoupled-Modulation.png" alt="Learning_Uncoupled-Modulation" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://link.springer.com/chapter/10.1007/978-3-031-19803-8_42">
                                Learning Uncoupled-Modulation CVAE for 3D Action-Conditioned Human Motion Synthesis
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Chongyang Zhong, Lei Hu, Zihao Zhang & Shihong Xia -->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089543322" target="_blank">Chongyang Zhong </a>,
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">lei hu </a>,
                            <a class="name" href="http://zihaozhang.tech/" target="_blank">Zihao Zhang </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                            
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://dblp.uni-trier.de/db/conf/eccv/index.html" target="_blank">
                                Computer Vision–ECCV 2022</a> ( LNCS,volume 13681)
                        <p class="projlinks">
                            [<a>Project Page</a>]
                            [<a href="papers/Learning Uncoupled-Modulation CVAE for 3D Action-Conditioned Human Motion Synthesis.pdf">Paper</a>]
                            [<a>Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!-- RAID-Net: Region-Aware Image Deblurring Network Under Guidance of the Image Blur Formulation -->
                <div id="RAID-Net" class="projitem">
                    <div id="RAID-Net" class="projthumbnail">
                        <a href="image/RAID-Net.gif" target="_self">
                            <img src="image/RAID-Net.gif" alt="RAID-Net" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/9840371">
                                RAID-Net: Region-Aware Image Deblurring Network Under Guidance of the Image Blur Formulation
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Lianjun Liao; Zihao Zhang; Shihong Xia -->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37086108917" target="_blank">Lianjun Liao </a>,
                            <a class="name" href="http://zihaozhang.tech/" target="_blank">Zihao Zhang </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6287639" target="_blank">
                                IEEE Access </a> (Volume: 10,26 July 2022)
                        </p>
                        <p class="projlinks">
                            [<a>Project Page</a>]
                            [<a href="papers/RAID-Net_ Region-Aware Image Deblurring Network Under Guidance of the Image Blur Formulation.pdf">Paper</a>]
                            [<a href="video/RAID-Net.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!-- Neural3Points: Learning to Generate Physically Realistic Full-body Motion for Virtual Reality Users -->
                <div id="Neural3Points" class="projitem">
                    <div id="Pose-aware" class="projthumbnail">
                        <a href="image/Neural3Points.png" target="_self">
                            <img src="image/Neural3Points.png" alt="Pose-aware" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <a class="papertitle" href="https://liamjing.github.io/Neural3Points/">
                                Neural3Points: Learning to Generate Physically Realistic Full-body Motion for Virtual Reality Users
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Yongjing Ye, Libin Liu, Lei Hu, Shihong Xia -->                            
                            <a class="name" href="https://liamjing.github.io/" target="_blank">Yongjing Ye </a>,
                            <a class="name" href="https://libliu.info/" target="_blank">Libin Liu</a>,
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">lei hu </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://www.scimagojr.com/journalsearch.php?q=25023&tip=sid" target="_blank">
                                Computer Graphics Forum</a> (Vol 41 Issue 8, Page 183-194 (SCA 2022))
                        </p>
                        <p class="projlinks">
                            [<a href="https://liamjing.github.io/Neural3Points/">Project Page</a>]
                            [<a href="https://HmmLab.github.io/works/papers/Neural3Points_Learning to Generate Physically Realistic Full-body.pdf">Paper</a>]
                            [<a href="https://www.youtube.com/watch?v=Y293EVW5jfM">YouTube</a>|<a href="https://www.bilibili.com/video/BV1KB4y1j7vg/">BiliBili</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                 <!-- Spatial-Temporal Gating-Adjacency GCN for Human Motion Prediction. -->
                <div id="Spatial-Temporal" class="projitem">                   
                    <div id="Spatial-Temporal" class="projthumbnail">
                        <a href="image/Spatial-Temporal.gif" target="_self">
                            <img src="image/Spatial-Temporal.gif" alt="Spatial-Temporal" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/9879222">
                                Spatial-Temporal Gating-Adjacency GCN for Human Motion Prediction
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Chongyang Zhong; Lei Hu; Zihao Zhang; Yongjing Ye; Shihong Xia -->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089543322" target="_blank">Chongyang Zhong </a>,
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">lei hu </a>,
                            <a class="name" href="http://zihaozhang.tech/" target="_blank">Zihao Zhang </a>,
                            <a class="name" href="https://liamjing.github.io/" target="_blank">Yongjing Ye </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/conhome/9878378/proceeding" target="_blank">
                                2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) </a> 
                        <p class="projlinks">
                            [<a href="https://github.com/Hmslab/Adapose">Project Page</a>]
                            [<a href="papers/Spatio-Temporal_Gating-Adjacency_GCN_for_Human_Motion_Prediction.pdf">Paper</a>]
                            [<a href="video/Spatio-Temporal.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!-- Active Colorization for Cartoon Line Drawings. -->
                <div id="Active-Colorization" class="projitem">                   
                    <div id="Active-Colorization" class="projthumbnail">
                        <a href="image/Active-Colorization.gif" target="_self">
                            <img src="image/Active-Colorization.gif" alt="Active-Colorization" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/9143503">
                                Active Colorization for Cartoon Line Drawings
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Shu-Yu Chen; Jia-Qi Zhang; Lin Gao; Yue He; Shihong Xia; Min Shi; Fang-Lue Zhang-->
                            <a class="name" href="http://people.geometrylearning.com/csy/" target="_blank">Shu-Yu Chen </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37087232590" target="_blank">Jia-Qi Zhang</a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089217482/" target="_blank">Yue He </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089216907" target="_blank">Min Shi </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/38474069400" target="_blank">Fang-Lue Zhang </a>
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                IEEE Transactions on Visualization and Computer Graphics </a> ( Volume: 28, Issue: 2, 01 February 2022)
                        <p class="projlinks">
                            [<a>Project Page</a>]
                            [<a href="papers/Active_Colorization_for_Cartoon_Line_Drawings.pdf">Paper</a>]
                            [<a href="video/Active-Colorization.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>              
                
                <div id="projects" class="section"> [2021] </div>
                <!--Combining Recurrent Neural Networks and Adversarial Training for Human Motion Synthesis and Control -->
                <div id="Combining_Recurrent" class="projitem">                   
                    <div id="Combining_Recurrent" class="projthumbnail">
                        <a href="image/Combining_Recurrent.gif" target="_self">
                            <img src="image/Combining_Recurrent.gif" alt="Combining_Recurrent" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/8826012/">
                                Combining Recurrent Neural Networks and Adversarial Training for Human Motion Synthesis and Control
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Zhiyong Wang; Jinxiang Chai; Shihong Xia -->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088563961" target="_blank">Zhiyong Wang </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37396663100" target="_blank">Jinxiang Chai </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                            
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                IEEE Transactions on Visualization and Computer Graphics</a> ( Volume: 27, Issue: 1, 01 January 2021)
                        <p class="projlinks">
                            [<a>Project Page</a>]
                            [<a href="papers/Combining_Recurrent_Neural_Networks_and_Adversarial_Training_for_Human_Motion_Synthesis_and_Control.pdf">Paper</a>]
                            [<a href="video/Combining_Recurrent.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--Realtime and Accurate 3D Eye Gaze Capture with DCNN-Based Iris and Pupil Segmentation. -->
                <div id="Realtime_and_Accurate_3D_Eye" class="projitem">                   
                    <div id="Realtime_and_Accurate_3D_Eye" class="projthumbnail">
                        <a href="image/Realtime_and_Accurate_3D_Eye.gif" target="_self">
                            <img src="image/Realtime_and_Accurate_3D_Eye.gif" alt="Realtime_and_Accurate_3D_Eye" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/8818661">
                                Realtime and Accurate 3D Eye Gaze Capture with DCNN-Based Iris and Pupil Segmentation
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Zhiyong Wang; Jinxiang Chai; Shihong Xia-->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088563961" target="_blank">Zhiyong Wang </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37396663100" target="_blank">Jinxiang Chai </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>       
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                IEEE Transactions on Visualization and Computer Graphics</a> (Volume: 27, Issue: 1, 01 January 2021)
                        <p class="projlinks">
                            [<a>Project Page</a>]
                            [<a href="papers/Realtime_and_Accurate_3D_Eye_Neural_Networks_and_Adversarial_Training_for_Human_Motion_Synthesis_and_Control.pdf">Paper</a>]
                            [<a>Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--Sparse Data Driven Mesh Deformation -->
                <div id="Sparse_Data" class="projitem">                   
                    <div id="Sparse_Data" class="projthumbnail">
                        <a href="image/Sparse_Data.gif" target="_self">
                            <img src="image/Sparse_Data.gif" alt="Sparse_Data" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/8839414">
                                Sparse Data Driven Mesh Deformation
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Lin Gao; Yu-Kun Lai; Jie Yang; Ling-Xiao Zhang; Shihong Xia; Leif Kobbelt -->
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088563961" target="_blank">Yu-Kun Lai </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088450605" target="_blank">Jie Yang </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088655587" target="_blank">Ling-Xiao Zhang </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37266790600" target="_blank">Leif Kobbelt </a>
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                IEEE Transactions on Visualization and Computer Graphics</a> (Volume: 27, Issue: 3, 01 March 2021)
                        <p class="projlinks">
                            [<a>Project Page</a>]
                            [<a href="papers/Sparse_Data_Neural_Networks_and_Adversarial_Training_for_Human_Motion_Synthesis_and_Control.pdf">Paper</a>]
                            [<a href="video/Sparse_Data.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--Sequential 3D Human Pose Estimation Using Adaptive Point Cloud Sampling Strategy. -->
                <div id="Sequential_3D_Human_Pose" class="projitem">                   
                    <div id="Sequential_3D_Human_Pose" class="projthumbnail">
                        <a href="image/Sequential_3D_Human_Pose.png" target="_self">
                            <img src="image/Sequential_3D_Human_Pose.png" alt="Sequential_3D_Human_Pose" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://www.ijcai.org/proceedings/2021/184">
                                Sequential 3D Human Pose Estimation Using Adaptive Point Cloud Sampling Strategy
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- 	Zihao Zhang, Lei Hu, Xiaoming Deng, Shihong Xia: -->
                            <a class="name" href="http://zihaozhang.tech/" target="_blank">Zihao Zhang </a>,                            
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">lei hu </a>,
                            <a class="name" href="https://teacher.ucas.ac.cn/~dengxm" target="_blank">Xiaoming Deng</a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ijcai-21.org/" target="_blank">
                                IJCAI,</a> (pp. 1330-1337. 2021)
                        <p class="projlinks">
                            [<a href="https://github.com/Hmslab/Adapose">Project Page</a>]
                            [<a href="papers/Sequential_3D_Human_Pose_Estimation_Using_Adaptive_Point_Cloud_Sampling.pdf">Paper</a>]
                            [<a href="https://ijcai-21.org/videos-slides/?video=6405">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--3D-TalkEmo: Learning to Synthesize 3D Emotional Talking Head. -->
                <div id="3D-TalkEmo" class="projitem">                   
                    <div id="3D-TalkEmo" class="projthumbnail">
                        <a href="image/3D-TalkEmo.png" target="_self">
                            <img src="image/3D-TalkEmo.png" alt="3D-TalkEmo" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://arxiv.org/abs/2104.12051">
                                3D-TalkEmo: Learning to Synthesize 3D Emotional Talking Head
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Qianyun Wang, Zhenfeng Fan, Shihong Xia -->
                            <a >Qianyun Wang </a>,                            
                            <a >Zhenfeng Fan </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                            
                            
                        </p>
                        <p class="projconference">
                            <a >
                                Computer Vision and Pattern Recognition</a> 
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/3D-TalkEmo_Learning_to_Synthesize_3D_Emotional_Talking_Head.pdf">Paper</a>]
                            [<a >Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--融合神经网络与数值计算的人体逆向运动学求解. -->
                <div id="融合" class="projitem">                   
                    <div id="融合" class="projthumbnail">
                        <a href="image/融合.png" target="_self">
                            <img src="image/融合.png" alt="融合" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://hlcdyy.github.io/publication/2021-01-01-MNN">
                                融合神经网络与数值计算的人体逆向运动学求解
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--胡磊, 张子豪, and 夏时洪-->
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">胡磊 </a>,
                            <a class="name" href="http://zihaozhang.tech/" target="_blank">张子豪 </a>, 
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">夏时洪</a>
                        </p>
                        <p class="projconference">
                            <a class="name"  target="_blank">
                                中国科学: 数学, 2021 </a> ( Volume: 26, Issue: 5, May 2020)
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/融合神经网络与数值计算的人体逆向.pdf">Paper</a>]
                            [<a href="https://www.bilibili.com/video/BV1A7411R7Lg/?spm_id_from=333.999.0.0&vd_source=cf809ecf45809233e7f000decbbeb176">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <div id="projects" class="section"> [2020] </div>
                <!-- 3D计算机图形学基础 -->
                <div id="3D" class="projitem">                   
                    <div id="3D" class="projthumbnail">
                        <a href="image/3D.jpg" target="_self">
                            <img src="image/3D.jpg" alt="3D" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            
                            <a class="papertitle" href="http://geometrylearning.com/DeepFaceDrawing/">
                                3D计算机图形学基础
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--[美] 史蒂夫·J.戈特勒(Steven J.Gortler)著; 夏时洪, 高林译-->
                            <a class="name" target="_blank">[美] 史蒂夫·J.戈特勒(Steven J.Gortler)著 </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">夏时洪 </a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">高林</a>译
                            

                        </p>
                        <p  class="projconference">
                            <a >清华大学出版社</a> 
                        <p class="projlinks">                           
                            <a href="https://shop16345114.m.youzan.com/wscgoods/detail/3ep8ji45sa1pe?scan=1&activity=none&from=kdt&qr=directgoods_844959919&shopAutoEnter=1&sf=wx_sm&is_share=1&share_cmpt=native_wechat&from_uuid=5c498aca-ac16-ca2a-9c0c-8f411ed389d4&redirect_count=1"> <链接> </a>
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!-- DeepFaceDrawing: deep generation of face images from sketches -->
                <div id="DeepFaceDrawing" class="projitem">                   
                    <div id="DeepFaceDrawing" class="projthumbnail">
                        <a href="image/DeepFaceDrawing.jpg" target="_self">
                            <img src="image/DeepFaceDrawing.jpg" alt="DeepFaceDrawing" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            
                            <a class="papertitle" href="http://geometrylearning.com/DeepFaceDrawing/">
                                DeepFaceDrawing: deep generation of face images from sketches
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Shu-Yu Chen#, Wanchao Su#, Lin Gao, Shihong Xia, Hongbo Fu -->
                            <a class="name" href="http://people.geometrylearning.com/csy/" target="_blank">Shu-Yu Chen </a>,
                            <a >Wanchao Su </a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37626240300" target="_blank">Hongbo Fu </a>
                        </p>
                        <p herf="https://www.scimagojr.com/journalsearch.php?q=24972&tip=sid" class="projconference">
                            <a >ACM Transactions on Graphics</a> (SIGGRAPH 2020), 2020, 39(4)
                        <p class="projlinks">
                            [<a href="http://geometrylearning.com/DeepFaceDrawing/">Project Page</a>]
                            [<a href="papers/DeepFaceDrawing_Deep Generation of Face Images from Sketches.pdf">Paper</a>]
                            [<a href="http://geometrylearning.com/DeepFaceDrawing/video/DeepFaceDrawing-video.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!-- Deep Line Art Video Colorization with a Few References -->
                <div id="Deep_Line_Art" class="projitem">                   
                    <div id="Deep_Line_Art" class="projthumbnail">
                        <a href="image/Deep_Line_Art.png" target="_self">
                            <img src="image/Deep_Line_Art.png" alt="Deep_Line_Art" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            
                            <a class="papertitle" href="https://arxiv.org/abs/2003.10685">
                                Deep Line Art Video Colorization with a Few References
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Min Shi, Jia-Qi Zhang, Shu-Yu Chen, Lin Gao, Yu-Kun Lai, Fang-Lue Zhang-->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089216907" target="_blank">Min Shi </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37087232590" target="_blank">Jia-Qi Zhang</a>,
                            <a class="name" href="http://people.geometrylearning.com/csy/" target="_blank">Shu-Yu Chen </a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088563961" target="_blank">Yu-Kun Lai </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/38474069400" target="_blank">Fang-Lue Zhang </a>
                        </p>
                        <p  class="projconference">
                            <a >Computer Vision and Pattern Recognition</a>
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/Deep Line Art Video Colorization with a Few References.pdf">Paper</a>]
                            [<a href="https://www.bilibili.com/video/av499191000/?vd_source=cf809ecf45809233e7f000decbbeb176">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--Weakly Supervised Adversarial Learning for 3D Human Pose Estimation from Point Clouds. -->
                <div id="Weakly_Supervised" class="projitem">                   
                    <div id="Weakly_Supervised" class="projthumbnail">
                        <a href="image/Weakly_Supervised.png" target="_self">
                            <img src="image/Weakly_Supervised.png" alt="Weakly_Supervised" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/8998337">
                                Weakly Supervised Adversarial Learning for 3D Human Pose Estimation from Point Clouds
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Zihao Zhang; Lei Hu; Xiaoming Deng; Shihong Xia-->
                            <a class="name" href="http://zihaozhang.tech/" target="_blank">Zihao Zhang </a>,                            
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">lei hu </a>,
                            <a class="name" href="https://teacher.ucas.ac.cn/~dengxm" target="_blank">Xiaoming Deng</a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                IEEE Transactions on Visualization and Computer Graphics </a> ( Volume: 26, Issue: 5, May 2020)
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/Weakly Supervised Adversarial Learning for 3D Human Pose Estimation from Point Clouds.pdf">Paper</a>]
                            [<a href="https://www.bilibili.com/video/BV1A7411R7Lg/?spm_id_from=333.999.0.0&vd_source=cf809ecf45809233e7f000decbbeb176">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <div id="projects" class="section"> [2019] </div>
                <!--Temporal Upsampling of Depth Maps Using a Hybrid Camera. -->
                <div id="Temporal_Upsamplinge" class="projitem">                   
                    <div id="Temporal_Upsamplinge" class="projthumbnail">
                        <a href="image/Temporal_Upsamplinge.jpg" target="_self">
                            <img src="image/Temporal_Upsamplinge.jpg" alt="Temporal_Upsamplinge" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/8307258">
                                Temporal Upsampling of Depth Maps Using a Hybrid Camera
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Mingze Yuan, Lin Gao*, Hongbo Fu, Shihong Xia-->
                            <a class="name" target="_blank">Mingze Yuan </a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao* </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37626240300" target="_blank">Hongbo Fu </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                            
                            
                        </p>
                        <p class="projconference">
                            <a >IEEE Transactions on Visualization and Computer Graphics</a> (IEEE TVCG), 2019, 25(3), 1591-1602
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/Temporal Upsampling of Depth Maps Using a Hybrid Camera.pdf">Paper</a>]
                            [<a href="video/Temporal_Upsamplinge.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <div id="projects" class="section"> [2018] </div>
                 <!--Real-Time 3D Face Reconstruction and Gaze Tracking for Virtual Reality. -->
                <div id="Real-Time_3D_Face" class="projitem">                   
                    <div id="Real-Time_3D_Face" class="projthumbnail">
                        <a href="image/Real-Time_3D_Face.jpg" target="_self">
                            <img src="image/Real-Time_3D_Face.jpg" alt="Real-Time_3D_Face" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/8818661">
                                Real-Time 3D Face Reconstruction and Gaze Tracking for Virtual Reality
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Shu-Yu Chen, Lin Gao*, Yu-Kun Lai, Paul L. Rosin, Shihong Xia-->
                            <a class="name" href="http://people.geometrylearning.com/csy/" target="_blank">Shu-Yu Chen </a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao* </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088563961" target="_blank">Yu-Kun Lai </a>,
                            <a >Paul L. Rosin </a>, 
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                            
                            
                        </p>
                        <p class="projconference">
                            <a >IEEE Conference on Virtual Reality</a>(IEEE VR 2018)
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/Real-Time 3D Face Reconstruction and Gaze Tracking for Virtual Reality.pdf">Paper</a>]
                            [<a href="video/Real-Time_3D_Face.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--Variational Autoencoders for Deforming 3D Mesh Models. -->
                <div id="Variational_Autoencoderse" class="projitem">       
                    <div id="Variational_Autoencoderse" class="projthumbnail">
                        <a href="image/Variational_Autoencoderse.png" target="_self">
                            <img src="image/Variational_Autoencoderse.png" alt="Variational_Autoencoderse" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/8578710/">
                                Variational Autoencoders for Deforming 3D Mesh Models
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Qingyang Tan;Lin Gao;Yu-Kun Lai;Shihong Xia-->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37086567528" target="_blank">Qingyang Tan </a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088563961" target="_blank">Yu-Kun Lai </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                        </p>
                        <p  herf="https://ieeexplore.ieee.org/xpl/conhome/8576498/proceeding" class="projconference">
                            <a >2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</a>
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/Variational Autoencoders for Deforming 3D Mesh Models.pdf">Paper</a>]
                            [<a >Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!-- Cascaded 3D Full-Body Pose Regression from Single Depth Image at 100 FPS -->
                <div id="Cascaded_3D_Full-Body" class="projitem">                   
                    <div id="Cascaded_3D_Full-Body" class="projthumbnail">
                        <a href="image/Cascaded_3D_Full-Body.png" target="_self">
                            <img src="image/Cascaded_3D_Full-Body.png" alt="Cascaded_3D_Full-Body" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="http://zihaozhang.tech/wp-content/uploads/2021/06/VR2018.pdf">
                                Cascaded 3D Full-Body Pose Regression from Single Depth Image at 100 FPS
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Xia Shihong, Zihao Zhang, and Le Su-->
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            <a class="name" href="http://zihaozhang.tech/" target="_blank">Zihao Zhang </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37086446969" target="_blank">Le Su </a>
                        </p>
                        <p class="projconference">
                            <a >IEEE Virtual Reality conference </a>(2018): 431-438.
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/Cascaded 3D Full-Body Pose Regression from Single Depth Image at 100 FPS.pdf">Paper</a>]
                            [<a >Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>
                
                <div id="projects" class="section"> [2017] </div>
                <!--Data-Driven Shape Interpolation and Morphing Editing. -->
                <div id="Data-Driven_Shape" class="projitem">                   
                    <div id="Data-Driven_Shape" class="projthumbnail">
                        <a href="image/Data-Driven_Shape.png" target="_self">
                            <img src="image/Data-Driven_Shape.png" alt="Data-Driven_Shape" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/8818661">
                                Data-Driven Shape Interpolation and Morphing Editing
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Lin Gao, Shu-Yu Chen, Yu-Kun Lai, Shihong Xia:-->
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao* </a>,
                            <a class="name" href="http://people.geometrylearning.com/csy/" target="_blank">Shu-Yu Chen </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088563961" target="_blank">Yu-Kun Lai </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                        </p>
                        <p class="projconference">
                            <a >Computer Graphics Forum 2017</a>
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/Data-Driven Shape Interpolation and Morphing Editing.pdf">Paper</a>]
                            [<a href="video/Data-Driven_Shape.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--Rigidity controllable as-rigid-as-possible shape deformation. -->
                 <div id="Rigidity_controllable" class="projitem">                   
                    <div id="Rigidity_controllable" class="projthumbnail">
                        <a href="image/Rigidity_controllable.png" target="_self">
                            <img src="image/Rigidity_controllable.png" alt="Rigidity_controllable" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle">
                                Rigidity controllable as-rigid-as-possible shape deformation
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Shu-Yu Chen, Lin Gao*, Yu-Kun Lai, Shihong Xia:-->
                            <a class="name" href="http://people.geometrylearning.com/csy/" target="_blank">Shu-Yu Chen </a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao* </a>,                           
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088563961" target="_blank">Yu-Kun Lai </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                        </p>
                        <p class="projconference">
                            <a >Graphical Models 2017</a>
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/Rigidity controllable as-rigid-as-possible shape deformation.pdf">Paper</a>]
                            [<a >Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>
                
                <!--Individual 3D Model Estimation for Realtime Human Motion Capture. -->
                <div id="Individual_3D_Model" class="projitem">                   
                    <div id="Individual_3D_Model" class="projthumbnail">
                        <a href="image/Individual_3D_Model.gif" target="_self">
                            <img src="image/Individual_3D_Model.gif" alt="Individual_3D_Model" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/8719117">
                                Individual 3D Model Estimation for Realtime Human Motion Capture
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Lianjun Liao; Le Su; Shihong Xia-->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37086108917" target="_blank">Lianjun Liao </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37086446969" target="_blank">Le Su </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                        </p>
                        <p class="projconference">
                            <a href="https://ieeexplore.ieee.org/xpl/conhome/8716287/proceeding">
                                2017 International Conference on Virtual Reality and Visualization (ICVRV)</a>
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/Individual_3D_Model_Estimation_for_Realtime_Human_Motion_Capture.pdf">Paper</a>]
                            [<a >Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <div id="projects" class="section"> [2016] </div>
                <!-- Data-driven Inverse Dynamics for Human Motion -->
                <div id="Data-driven_Inverse" class="projitem">                   
                    <div id="Data-driven_Inverse" class="projthumbnail">
                        <a href="image/Data-driven_Inverse.jpg" target="_self">
                            <img src="image/Data-driven_Inverse.jpg" alt="Data-driven_Inverse" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://dl.acm.org/doi/epdf/10.1145/2980179.2982440">
                                Data-driven Inverse Dynamics for Human Motion
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Xiaolei Lv, Jinxiang Chai, Shihong Xia-->
                            <a class="name" target="_blank">Xiaolei Lv </a>,
                            <a class="name" target="_blank">Jinxiang Chai </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>                          
                            
                        </p>
                        <p class="projconference">
                            <a >SIGGRAPH Asia 2016</a>
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/Data-driven Inverse Dynamics for Human Motion.pdf">Paper</a>]
                            [<a href="video/Data-driven_Inverse.avi">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--Realtime 3D Eye Gaze Animation Using a Single RGB Camera. -->
                <div id="Realtime_3D_Eye" class="projitem">                   
                    <div id="Realtime_3D_Eye" class="projthumbnail">
                        <a href="image/Realtime_3D_Eye.png" target="_self">
                            <img src="image/Realtime_3D_Eye.png" alt="Realtime_3D_Eye" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/8818661">
                                Realtime 3D Eye Gaze Animation Using a Single RGB Camera
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Congyi Wang, Fuhao Shi, Shihong Xia, Jinxiang Chai-->
                            <a class="name" target="_blank">Congyi Wang </a>,
                            <a class="name" target="_blank">Fuhao Shi </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            <a class="name" target="_blank">Jinxiang Chai </a>
                            
                        </p>
                        <p class="projconference">
                            <a >ACM Transactions on Graphics (SIGGRAPH), 2016</a>
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/Realtime 3D Eye Gaze Animation Using a Single RGB Camera.pdf">Paper</a>]
                            [<a href="video/Realtime_3D_Eye.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--Efficient and Flexible Deformation Representation for Data-Driven Surface Modeling. -->
                <div id="Efficient_and_Flexible" class="projitem">                   
                    <div id="Efficient_and_Flexible" class="projthumbnail">
                        <a href="image/Efficient_and_Flexible.png" target="_self">
                            <img src="image/Efficient_and_Flexible.png" alt="Efficient_and_Flexible" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://www.researchgate.net/publication/305743983_Efficient_and_Flexible_Deformation_Representation_for_Data-Driven_Surface_Modeling">
                                Efficient and Flexible Deformation Representation for Data-Driven Surface Modeling
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Lin Gao, Yu-Kun Lai, Dun Liang, Shu-Yu Chen, Shihong Xia-->
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao</a>,  
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088563961" target="_blank">Yu-Kun Lai </a>,    
                            <a class="name" target="_blank">Dun Liang </a>,  
                            <a class="name" href="http://people.geometrylearning.com/csy/" target="_blank">Shu-Yu Chen </a>,              
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>
                            
                        </p>
                        <p class="projconference">
                            <a >ACM Transactions on Graphics (SIGGRAPH), 2016</a>
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/Efficient and Flexible Deformation Representation for Data-Driven Surface Modeling.pdf">Paper</a>]
                            [<a href="video/Efficient_and_Flexible.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>
                
                <div id="projects" class="section"> [2015] </div>
                <!--Realtime Style Transfer for Unlabeled Heterogeneous Human Motion -->
                <div id="Realtime_Style_Transfer" class="projitem">                   
                    <div id="Realtime_Style_Transfer" class="projthumbnail">
                        <a href="image/Realtime_Style_Transfer.jpg" target="_self">
                            <img src="image/Realtime_Style_Transfer.jpg" alt="Realtime_Style_Transfer" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://www.researchgate.net/publication/283889632_Realtime_Style_Transfer_for_Unlabeled_Heterogeneous_Human_Motion">
                                Realtime Style Transfer for Unlabeled Heterogeneous Human Motion
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Shihong Xia,Congyi Wang,Jinxiang Chai,Jessica Hodgins-->
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            <a class="name" target="_blank">Congyi Wang </a>,
                            <a class="name" target="_blank">Jinxiang Chai </a>,                            
                            <a class="name" target="_blank">Jessica Hodgins </a>
                            
                        </p>
                        <p class="projconference">
                            <a >ACM Transactions on Graphics (SIGGRAPH), 2015</a>
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/Realtime Style Transfer for Unlabeled Heterogeneous Human Motion.pdf">Paper</a>]
                            [<a href="video/Realtime_Style_Transfer.avi">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!-- 视频驱动的语义表情基动画方法 -->
                <div id="Video-Driven" class="projitem">                   
                    <div id="Video-Driven" class="projthumbnail">
                        <a href="image/Video-Driven.jpg" target="_self">
                            <img src="image/Video-Driven.jpg" alt="Video-Driven" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" >
                                视频驱动的语义表情基动画方法
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--王涵, 夏时洪-->
                            <a class="name" target="_blank">王涵 </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">夏时洪 </a>
                            
                            
                        </p>
                        <p class="projconference">
                            <a>计算机辅助设计与图形学学报. 27(5): 873-882, 2015.</a>（中国计算机图形学大会 2014）
                        </p>
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a >Paper</a>]
                            [<a >Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>
            </div>            
        </div>

        <!-- <div id="activity" class="section">
            <div id="activitysectitle" class="sectitle">[Professional Activities]</div>
            <div id="activities" class="seccontent">
                <div class="activitytype">
                    Program Committee:
                    <ul>
                        <li>ACM SIGGRAPH North America 2019, 2020, Asia 2022</li>
                        <li>Pacific Graphics 2018, 2019, 2022</li>
                        <li>SCA 2015-2019, 2021-2023</li>
                        <li>MIG 2014, 2016-2019, 2022</li>
                        <li>Eurographics Short Papers 2020, 2021</li>
                        <li>SIGGRAPH Asia 2014 Posters and Technical Briefs</li>
                        <li>CASA (Computer Animation and Social Agents) 2017, 2023</li>
                        <li>Graphics Interface 2023</li>
                        <li>CAD/Graphics 2017, 2019</li>
                    </ul>
                </div> 
                <div class="activitytype">
                    Paper Reviewing:
                    <ul>
                        <li>SIGGRAPH NA/Asia</li>
                        <li>ACM Transactions on Graphics (TOG)</li>
                        <li>IEEE Transactions on Visualization and Computer Graphics (TVCG)</li>
                        <li>International Conference on Computer Vision (ICCV)</li>
                        <li>Eurographics (Eupopean Association for Computer Graphics)</li>
                        <li>Pacific Graphics</li>
                        <li>Computer Graphics Forum</li>
                        <li>IEEE International Conference on Robotics and Automation (ICRA)</li>
                        <li>ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA)</li>
                        <li>ACM SIGGRAPH Conference on Motion, Interaction and Games (MIG)</li>
                        <li>Computer Animation and Social Agents (CASA)</li>
                        <li>Graphics Interface</li>
                        <li>Computers & Graphics</li>
                        <li>Graphical Models</li>
                    </ul>
                </div> 
        </div>
        <div id="footing"></div>
    </div> -->
</body>
</html>