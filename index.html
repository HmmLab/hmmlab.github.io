
<!DOCTYPE HTML PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <title>HmmLab</title>
    <link rel="icon" href="favicon.ico" type="image/x-icon" />
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
    <link rel="bookmark" href="favicon.ico" type="image/x-icon" />
    <link rel="stylesheet" type="text/css" href="main.css" />
    <link rel="stylesheet" type="text/css" href="index.css" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-TXEVECV8DQ"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-TXEVECV8DQ');
    </script>
</head>
<body>
    <div id="wrapper">
        <div id="heading"></div>
        <div id="aboutme" class="section">
            <div id="aboutmesectitle" class="sectitle">[About lAB]</div>
            <div id="aboutmecontent" class="seccontent">
                <div id="mypic">
                    <!-- <a href="index.html">
                        <img src="imgs/me.jpg" alt="Libin Liu" />
                    </a> -->
                </div>
                <div id="myinfo">
                    <p id="myname">
                        HmmLab
                    </p>
                    <!-- <p id="myaffiliation">
                        Assistant Professor
                        <br /><br />
                        <a class="institute" href="https://www.cis.pku.edu.cn/" target="_blank">
                            School of Intelligence Science and Technology
                        </a>
                        <br />
                        <a class="institute" href="http://english.pku.edu.cn/" target="_blank">Peking University</a>
                    </p> -->
                    <!-- <p id="contacttitle"></p>
                    <p class="mycontact">
                        Email: libin.liu [at] pku.edu.cn
                    </p> -->
                </div>
                <div id="myintro">
                    <p>
                        实验室简介：<br/>
                        
                        <br />
                    </p>
                </div>
            </div>
        </div>

        <div id="project" class="section">
            <div id="projsectitle" class="sectitle">[Projects]</div>
            <div id="projects" class="seccontent">  
                <div id="projects" class="sectitle"> [2023] </div>
                <div id="Motif" class="projitem">
                    <div id="Motif" class="projthumbnail">
                        <a href="https://HmmLab.github.io/works/image/Motif.gif" target="_self">
                            <img src="https://HmmLab.github.io/works/image/Motif.gif" alt="Motif" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <a class="papertitle" href="https://github.com/wenyh1616/SAMotif-GCN">
                                Motif-GCNs With Local and Non-Local Temporal Blocks for Skeleton-Based Action Recognition.
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Yu-Hui Wen, Lin Gao, Hongbo Fu, Fang-Lue Zhang, Shihong Xia, Yong-Jin Liu -->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089016300" target="_blank">Yu-Hui </a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37626240300" target="_blank">Hongbo Fu </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/38474069400" target="_blank">Fang-Lue Zhang </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37279426700" target="_blank">Yong-Jin Liu</a>,
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank">
                                IEEE Transactions on Pattern Analysis and Machine Intelligence </a>( Volume: 45, Issue: 2, 01 February 2023)
                        </p>
                        <p class="projlinks">
                            [<a href="https://github.com/wenyh1616/SAMotif-GCN">Project Page</a>]
                            [<a href="https://HmmLab.github.io/works/papers/Motif-GCNs_With_Local_and_Non-Local_Temporal_Blocks_for_Skeleton-Based_Action_Recognition.pdf">Paper</a>]
                            <!-- [Video (<a href="https://www.youtube.com/watch?v=qy2MrNhsoIs">YouTube</a>|<a href="https://www.bilibili.com/video/BV1G24y1d7Tt">BiliBili</a>)] -->
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>
                
                <div id="Multiscale" class="projitem">
                    <div id="Multiscale" class="projthumbnail">
                        <a href="image/Multiscale Mesh.gif" target="_self">
                            <img src="image/Multiscale Mesh.gif" alt="Motif" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/9537699">
                                Multiscale Mesh Deformation Component Analysis With Attention-Based Autoencoders
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Jie Yang; Lin Gao; Qingyang Tan; Yi-Hua Huang; Shihong Xia; Yu-Kun Lai -->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088450605" target="_blank">Jie Yang </a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37086567528" target="_blank">Qingyang Tan </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089542762" target="_blank">Yi-Hua Huang </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/38575316500" target="_blank">Yu-Kun Lai</a>,
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                IEEE Transactions on Visualization and Computer Graphics</a> (Volume: 29, Issue: 2, 01 February 2023)
                        </p>
                        <p class="projlinks">
                            [Project Page]<!-- [<a href="https://github.com/wenyh1616/SAMotif-GCN">Project Page</a>] -->
                            [<a href="https://HmmLab.github.io/works/papers/Multiscale_Mesh_Deformation_Component_Analysis_wit.pdf">Paper</a>]
                            [<a href="video/Multiscale Mesh.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <div id="Pose-aware" class="projitem">
                    <div id="Pose-aware" class="projthumbnail">
                        <a href="himage/Pose-aware.png" target="_self">
                            <img src="image/Pose-aware.png" alt="Pose-aware" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://arxiv.org/abs/2306.08006">
                                Pose-aware Attention Network for Flexible Motion Retargeting by Body Part
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Lei Hu, Zihao Zhang, Chongyang Zhong, Boyuan Jiang, Shihong Xia -->
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">lei hu </a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao </a>,
                            <a class="name" href="http://zihaozhang.tech/" target="_blank">Zihao Zhang</a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089543322" target="_blank">Chongyang Zhong </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37086479093" target="_blank">Boyuan Jiang</a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                IEEE Transactions on Visualization and Computer Graphics</a> (Volume: 10,26 July 2022)
                        </p>
                        <p class="projlinks">
                            [<a href="https://github.com/hlcdyy/pan-motion-retargeting">Project Page</a>]
                            [<a href="https://HmmLab.github.io/works/papers/Pose-aware Attention Network for Flexible Motion Retargeting by Body Part.pdf">Paper</a>]
                            [<a href="https://youtu.be/oTAcxTtPXUg">YouTube</a>|<a href="https://www.bilibili.com/video/BV1bg4y1V7Xi/">BiliBili</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <div id="projects" class="sectitle"> [2022] </div>
                <div id="RAID-Net" class="projitem">
                    <div id="RAID-Net" class="projthumbnail">
                        <a href="image/RAID-Net.gif" target="_self">
                            <img src="image/RAID-Net.gif" alt="RAID-Net" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/9840371">
                                RAID-Net: Region-Aware Image Deblurring Network Under Guidance of the Image Blur Formulation
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Lianjun Liao; Zihao Zhang; Shihong Xia -->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37086108917" target="_blank">Lianjun Liao </a>,
                            <a class="name" href="http://zihaozhang.tech/" target="_blank">Zihao Zhang </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6287639" target="_blank">
                                IEEE Access </a> (Volume: 10,26 July 2022)
                        </p>
                        <p class="projlinks">
                            [<a>Project Page</a>]
                            [<a href="papers/RAID-Net_ Region-Aware Image Deblurring Network Under Guidance of the Image Blur Formulation.pdf">Paper</a>]
                            [<a href="video/RAID-Net.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!-- Neural3Points: Learning to Generate Physically Realistic Full-body Motion for Virtual Reality Users -->
                <div id="Neural3Points" class="projitem">
                    <div id="Pose-aware" class="projthumbnail">
                        <a href="image/Neural3Points.png" target="_self">
                            <img src="image/Neural3Points.png" alt="Pose-aware" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <a class="papertitle" href="https://liamjing.github.io/Neural3Points/">
                                Neural3Points: Learning to Generate Physically Realistic Full-body Motion for Virtual Reality Users
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Yongjing Ye, Libin Liu, Lei Hu, Shihong Xia -->                            
                            <a class="name" href="https://liamjing.github.io/" target="_blank">Yongjing Ye </a>,
                            <a class="name" href="https://libliu.info/" target="_blank">Libin Liu</a>,
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">lei hu </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://www.scimagojr.com/journalsearch.php?q=25023&tip=sid" target="_blank">
                                Computer Graphics Forum</a> (Vol 41 Issue 8, Page 183-194 (SCA 2022))
                        </p>
                        <p class="projlinks">
                            [<a href="https://liamjing.github.io/Neural3Points/">Project Page</a>]
                            [<a href="https://HmmLab.github.io/works/papers/Neural3Points_Learning to Generate Physically Realistic Full-body.pdf">Paper</a>]
                            [<a href="https://www.youtube.com/watch?v=Y293EVW5jfM">YouTube</a>|<a href="https://www.bilibili.com/video/BV1KB4y1j7vg/">BiliBili</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                 <!-- Spatial-Temporal Gating-Adjacency GCN for Human Motion Prediction. -->
                <div id="Spatial-Temporal" class="projitem">                   
                    <div id="Spatial-Temporal" class="projthumbnail">
                        <a href="image/Spatial-Temporal.gif" target="_self">
                            <img src="image/Spatial-Temporal.gif" alt="Spatial-Temporal" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/9879222">
                                Spatial-Temporal Gating-Adjacency GCN for Human Motion Prediction
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Chongyang Zhong; Lei Hu; Zihao Zhang; Yongjing Ye; Shihong Xia -->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089543322" target="_blank">Chongyang Zhong </a>,
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">lei hu </a>,
                            <a class="name" href="http://zihaozhang.tech/" target="_blank">Zihao Zhang </a>,
                            <a class="name" href="https://liamjing.github.io/" target="_blank">Yongjing Ye </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/conhome/9878378/proceeding" target="_blank">
                                2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) </a> 
                        <p class="projlinks">
                            [<a href="https://github.com/Hmslab/Adapose">Project Page</a>]
                            [<a href="papers/Spatio-Temporal_Gating-Adjacency_GCN_for_Human_Motion_Prediction.pdf">Paper</a>]
                            [<a href="video/Spatio-Temporal.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!-- Active Colorization for Cartoon Line Drawings. -->
                <div id="Active-Colorization" class="projitem">                   
                    <div id="Active-Colorization" class="projthumbnail">
                        <a href="image/Active-Colorization.gif" target="_self">
                            <img src="image/Active-Colorization.gif" alt="Active-Colorization" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/9143503">
                                Active Colorization for Cartoon Line Drawings
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Shu-Yu Chen; Jia-Qi Zhang; Lin Gao; Yue He; Shihong Xia; Min Shi; Fang-Lue Zhang-->
                            <a class="name" href="http://people.geometrylearning.com/csy/" target="_blank">Shu-Yu Chen </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37087232590" target="_blank">Jia-Qi Zhang</a>,
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089217482/" target="_blank">Yue He </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089216907" target="_blank">Min Shi </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/38474069400" target="_blank">Fang-Lue Zhang </a>,
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                IEEE Transactions on Visualization and Computer Graphics </a> ( Volume: 28, Issue: 2, 01 February 2022)
                        <p class="projlinks">
                            [<a>Project Page</a>]
                            [<a href="papers/Active_Colorization_for_Cartoon_Line_Drawings.pdf">Paper</a>]
                            [<a href="video/Active-Colorization.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>


                <!--Spatial-temporal modeling for prediction of stylized human motion. -->
                <div id="Spatio-temporal" class="projitem">                   
                    <div id="Spatio-temporal" class="projthumbnail">
                        <a href="image/Spatio-temporal.jpg" target="_self">
                            <img src="image/Spatio-temporal.jpg" alt="Spatio-temporal" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://www.sciencedirect.com/science/article/pii/S092523122201075X">
                                Spatial-temporal modeling for prediction of stylized human motion
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Chongyang Zhong, Lei Hu, Shihong Xia-->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089543322" target="_blank">Chongyang Zhong </a>,
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">lei hu </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://www.sciencedirect.com/journal/neurocomputing" target="_blank">
                                Neurocomputing </a> ( Volume 511, 28 October 2022, Pages 34-42)
                        <p class="projlinks">
                            [<a>Project Page</a>]
                            [<a href="papers/Spatial–temporal modeling for prediction of stylized human motion.pdf">Paper</a>]
                            [<a>Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--Learning Uncoupled-Modulation CVAE for 3D Action-Conditioned Human Motion Synthesis. -->
                <div id="Learning_Uncoupled-Modulation" class="projitem">                   
                    <div id="Learning_Uncoupled-Modulation" class="projthumbnail">
                        <a href="image/Learning_Uncoupled-Modulation.png" target="_self">
                            <img src="image/Learning_Uncoupled-Modulation.png" alt="Learning_Uncoupled-Modulation" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://link.springer.com/chapter/10.1007/978-3-031-19803-8_42">
                                Learning Uncoupled-Modulation CVAE for 3D Action-Conditioned Human Motion Synthesis
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Chongyang Zhong, Lei Hu, Zihao Zhang & Shihong Xia -->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37089543322" target="_blank">Chongyang Zhong </a>,
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">lei hu </a>,
                            <a class="name" href="http://zihaozhang.tech/" target="_blank">Zihao Zhang </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://dblp.uni-trier.de/db/conf/eccv/index.html" target="_blank">
                                Computer Vision–ECCV 2022</a> ( LNCS,volume 13681)
                        <p class="projlinks">
                            [<a>Project Page</a>]
                            [<a href="papers/Learning Uncoupled-Modulation CVAE for 3D Action-Conditioned Human Motion Synthesis.pdf">Paper</a>]
                            [<a>Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>
                
                <div id="projects" class="sectitle"> [2021] </div>
                <!--Combining Recurrent Neural Networks and Adversarial Training for Human Motion Synthesis and Control -->
                <div id="Combining_Recurrent" class="projitem">                   
                    <div id="Combining_Recurrent" class="projthumbnail">
                        <a href="image/Combining_Recurrent.gif" target="_self">
                            <img src="image/Combining_Recurrent.gif" alt="Combining_Recurrent" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/8826012/">
                                Combining Recurrent Neural Networks and Adversarial Training for Human Motion Synthesis and Control
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Zhiyong Wang; Jinxiang Chai; Shihong Xia -->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088563961" target="_blank">Zhiyong Wang </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37396663100" target="_blank">Jinxiang Chai </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                IEEE Transactions on Visualization and Computer Graphics</a> ( Volume: 27, Issue: 1, 01 January 2021)
                        <p class="projlinks">
                            [<a>Project Page</a>]
                            [<a href="papers/Combining_Recurrent_Neural_Networks_and_Adversarial_Training_for_Human_Motion_Synthesis_and_Control.pdf">Paper</a>]
                            [<a href="video/Combining_Recurrent.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--Realtime and Accurate 3D Eye Gaze Capture with DCNN-Based Iris and Pupil Segmentation. -->
                <div id="Realtime_and_Accurate_3D_Eye" class="projitem">                   
                    <div id="Realtime_and_Accurate_3D_Eye" class="projthumbnail">
                        <a href="image/Realtime_and_Accurate_3D_Eye.gif" target="_self">
                            <img src="image/Realtime_and_Accurate_3D_Eye.gif" alt="Realtime_and_Accurate_3D_Eye" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/8818661">
                                Realtime and Accurate 3D Eye Gaze Capture with DCNN-Based Iris and Pupil Segmentation
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Zhiyong Wang; Jinxiang Chai; Shihong Xia-->
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088563961" target="_blank">Zhiyong Wang </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37396663100" target="_blank">Jinxiang Chai </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                IEEE Transactions on Visualization and Computer Graphics</a> (Volume: 27, Issue: 1, 01 January 2021)
                        <p class="projlinks">
                            [<a>Project Page</a>]
                            [<a href="papers/Realtime_and_Accurate_3D_Eye_Neural_Networks_and_Adversarial_Training_for_Human_Motion_Synthesis_and_Control.pdf">Paper</a>]
                            [<a>Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--Sparse Data Driven Mesh Deformation -->
                <div id="Sparse_Data" class="projitem">                   
                    <div id="Sparse_Data" class="projthumbnail">
                        <a href="image/Sparse_Data.gif" target="_self">
                            <img src="image/Sparse_Data.gif" alt="Sparse_Data" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://ieeexplore.ieee.org/document/8839414">
                                Sparse Data Driven Mesh Deformation
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- Lin Gao; Yu-Kun Lai; Jie Yang; Ling-Xiao Zhang; Shihong Xia; Leif Kobbelt -->
                            <a class="name" href="http://geometrylearning.com/" target="_blank">Lin Gao </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088563961" target="_blank">Yu-Kun Lai </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088450605" target="_blank">Jie Yang </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37088655587" target="_blank">Ling-Xiao Zhang </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            <a class="name" href="https://ieeexplore.ieee.org/author/37266790600" target="_blank">Leif Kobbelt </a>,
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">
                                IEEE Transactions on Visualization and Computer Graphics</a> (Volume: 27, Issue: 3, 01 March 2021)
                        <p class="projlinks">
                            [<a>Project Page</a>]
                            [<a href="papers/Sparse_Data_Neural_Networks_and_Adversarial_Training_for_Human_Motion_Synthesis_and_Control.pdf">Paper</a>]
                            [<a href="video/Sparse_Data.mp4">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--Sequential 3D Human Pose Estimation Using Adaptive Point Cloud Sampling Strategy. -->
                <div id="Sequential_3D_Human_Pose" class="projitem">                   
                    <div id="Sequential_3D_Human_Pose" class="projthumbnail">
                        <a href="image/Sequential_3D_Human_Pose.png" target="_self">
                            <img src="image/Sequential_3D_Human_Pose.png" alt="Sequential_3D_Human_Pose" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://www.ijcai.org/proceedings/2021/184">
                                Sequential 3D Human Pose Estimation Using Adaptive Point Cloud Sampling Strategy
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!-- 	Zihao Zhang, Lei Hu, Xiaoming Deng, Shihong Xia: -->
                            <a class="name" href="http://zihaozhang.tech/" target="_blank">Zihao Zhang </a>,                            
                            <a class="name" href="https://hlcdyy.github.io/" target="_blank">lei hu </a>,
                            <a class="name" href="https://teacher.ucas.ac.cn/~dengxm" target="_blank">Xiaoming Deng</a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            
                            
                        </p>
                        <p class="projconference">
                            <a class="name" href="https://ijcai-21.org/" target="_blank">
                                IJCAI,</a> (pp. 1330-1337. 2021)
                        <p class="projlinks">
                            [<a href="https://github.com/Hmslab/Adapose">Project Page</a>]
                            [<a href="papers/Sequential_3D_Human_Pose_Estimation_Using_Adaptive_Point_Cloud_Sampling.pdf">Paper</a>]
                            [<a href="https://ijcai-21.org/videos-slides/?video=6405">Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                <!--3D-TalkEmo: Learning to Synthesize 3D Emotional Talking Head. -->
                <div id="3D-TalkEmo" class="projitem">                   
                    <div id="3D-TalkEmo" class="projthumbnail">
                        <a href="image/3D-TalkEmo.png" target="_self">
                            <img src="image/3D-TalkEmo.png" alt="3D-TalkEmo" />
                        </a>
                    </div>
                    <div class="projintro">
                        <p class="projtitle">
                            <!-- 项目网址,目前为论文网址 -->
                            <a class="papertitle" href="https://arxiv.org/abs/2104.12051">
                                3D-TalkEmo: Learning to Synthesize 3D Emotional Talking Head
                            </a>
                        </p>
                        <!-- 作者 -->
                        <p class="projauthor">
                            <!--Qianyun Wang, Zhenfeng Fan, Shihong Xia -->
                            <a >Qianyun Wang </a>,                            
                            <a >Zhenfeng Fan </a>,
                            <a class="name" href="https://people.ucas.ac.cn/~0004712" target="_blank">Shihong Xia </a>,
                            
                            
                        </p>
                        <p class="projconference">
                            <a >
                                Computer Vision and Pattern Recognition</a> 
                        <p class="projlinks">
                            [<a >Project Page</a>]
                            [<a href="papers/3D-TalkEmo_Learning_to_Synthesize_3D_Emotional_Talking_Head.pdf">Paper</a>]
                            [<a >Video</a>]
                            <!-- [<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page">Code</a>] (Coming soon...) -->
                        </p>
                    </div>
                </div>

                
            </div>            
        </div>

        <!-- <div id="activity" class="section">
            <div id="activitysectitle" class="sectitle">[Professional Activities]</div>
            <div id="activities" class="seccontent">
                <div class="activitytype">
                    Program Committee:
                    <ul>
                        <li>ACM SIGGRAPH North America 2019, 2020, Asia 2022</li>
                        <li>Pacific Graphics 2018, 2019, 2022</li>
                        <li>SCA 2015-2019, 2021-2023</li>
                        <li>MIG 2014, 2016-2019, 2022</li>
                        <li>Eurographics Short Papers 2020, 2021</li>
                        <li>SIGGRAPH Asia 2014 Posters and Technical Briefs</li>
                        <li>CASA (Computer Animation and Social Agents) 2017, 2023</li>
                        <li>Graphics Interface 2023</li>
                        <li>CAD/Graphics 2017, 2019</li>
                    </ul>
                </div> 
                <div class="activitytype">
                    Paper Reviewing:
                    <ul>
                        <li>SIGGRAPH NA/Asia</li>
                        <li>ACM Transactions on Graphics (TOG)</li>
                        <li>IEEE Transactions on Visualization and Computer Graphics (TVCG)</li>
                        <li>International Conference on Computer Vision (ICCV)</li>
                        <li>Eurographics (Eupopean Association for Computer Graphics)</li>
                        <li>Pacific Graphics</li>
                        <li>Computer Graphics Forum</li>
                        <li>IEEE International Conference on Robotics and Automation (ICRA)</li>
                        <li>ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA)</li>
                        <li>ACM SIGGRAPH Conference on Motion, Interaction and Games (MIG)</li>
                        <li>Computer Animation and Social Agents (CASA)</li>
                        <li>Graphics Interface</li>
                        <li>Computers & Graphics</li>
                        <li>Graphical Models</li>
                    </ul>
                </div> 
        </div>
        <div id="footing"></div>
    </div> -->
</body>
</html>